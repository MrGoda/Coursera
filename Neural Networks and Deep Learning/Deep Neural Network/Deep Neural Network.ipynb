{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2507fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package & data imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "## Read data\n",
    "#data_file = 'data/binary_classification.csv'\n",
    "#raw_data = pd.read_csv(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2cec4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building blocks of a DNN\n",
    "\n",
    "\n",
    "# Function to construct a deep neural network\n",
    "# Input the training data X and Y, return the trained parameters which represent the model trained\n",
    "def deep_NN(X, Y, layer_dims, max_iter=500, learning_rate=0.01):\n",
    "    \n",
    "    # Insert the input (layer 0) into the layer_dims\n",
    "    layer_dims.insert(0, X.shape[0])\n",
    "    \n",
    "    parameters = initialize(layer_dims)\n",
    "    #print(parameters)\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        cache, cost = forward_prop(X, Y, parameters, activation='tanh')\n",
    "        gradients = back_prop(Y, cache, parameters, activation='tanh')\n",
    "        \n",
    "        # Update parameters\n",
    "        l = int(len(parameters)/2)\n",
    "\n",
    "        for j in range(1, l, 1):\n",
    "            parameters['W'+str(j)] = parameters['W'+str(j)] - learning_rate * gradients['dW'+str(j)]\n",
    "            parameters['b'+str(j)] = parameters['b'+str(j)] - learning_rate * gradients['db'+str(j)]\n",
    "    \n",
    "        # Produce an output every 100 iterations\n",
    "        if i % 1 == 0:\n",
    "            print('Current iteration is', i, 'and the current cost is', cost)\n",
    "            \n",
    "            \n",
    "# Function to initialize the parameter for a DNN\n",
    "# Input the number of layers for each layers and output a set of initialized parameters\n",
    "def initialize(layer_dims):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims), 1):\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Function to conduct forward propagation in a DNN\n",
    "# May customize the activation function choosing between tanh and relu activation (except the last layer)\n",
    "# Input is the X and parameters\n",
    "# Output A and Z (as cache) for backward propagation\n",
    "def forward_prop(X, Y, parameters, activation='tanh'):\n",
    "    l = int(len(parameters)/2)\n",
    "    m = X.shape[0]\n",
    "    cache = {'A0': X}\n",
    "    \n",
    "    cache['Z1'] = np.matmul(parameters['W1'], X) + parameters['b1']\n",
    "    if activation == 'tanh':\n",
    "        cache['A1'] = tanh_activation(cache['Z1'])\n",
    "    elif activation == 'relu':\n",
    "        cache['A1'] = relu_activation(cache['Z1'])\n",
    "    else:\n",
    "        print('Incorrect activation value, process should abort.')\n",
    "        \n",
    "    for i in range(2, l, 1):\n",
    "        cache['Z'+str(i)] = np.matmul(parameters['W'+str(i)], cache['A'+str(i-1)]) + parameters['b'+str(i)]\n",
    "        if activation == 'tanh':\n",
    "            cache['A'+str(i)] = tanh_activation(cache['Z'+str(i)])\n",
    "        elif activation == 'relu':\n",
    "            cache['A'+str(i)] = relu_activation(cache['Z'+str(i)])\n",
    "    \n",
    "    # Last layer\n",
    "    cache['Z'+str(l)] = np.matmul(parameters['W'+str(l)], cache['A'+str(l-1)]) + parameters['b'+str(l)]\n",
    "    cache['A'+str(l)] = sigmoid_activation(cache['Z'+str(l)])\n",
    "    \n",
    "    #print(cache['A'+str(l)])\n",
    "    cost = - 1/m * np.sum(np.multiply(Y, np.log(cache['A'+str(l)])) + np.multiply((1 - Y), np.log(1 - cache['A'+str(l)])))\n",
    "    \n",
    "    return cache, cost\n",
    "\n",
    "\n",
    "# Function to conduct back propagation in a DNN\n",
    "# May customize the activation function choosing between tanh and relu activation (except the last layer)\n",
    "# Inputs are Y, cache (Z and A value), parameter, and activation choices\n",
    "# Output derivative\n",
    "def back_prop(Y, cache, parameters, activation='tanh'):\n",
    "    l = int(len(parameters)/2)\n",
    "    m = X.shape[0]\n",
    "    gradients = {}\n",
    "    \n",
    "    # Backpropagate last layer assuming sigmoid activation function in the last layer\n",
    "    dZ = cache['A'+str(l)] - Y # Gradient of sigmoid activation\n",
    "    dW = 1/m * np.matmul(dZ, cache['A'+str(l-1)].T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    gradients['dW'+str(l)] = dW\n",
    "    gradients['db'+str(l)] = db\n",
    "    \n",
    "    dA = np.matmul(parameters['W'+str(l)].T, dZ)\n",
    "    for i in range(l-1, 0, -1):\n",
    "        # dZ at current layer\n",
    "        if activation == 'tanh':\n",
    "            dZ = np.multiply(dA, tanh_derivative(cache['Z'+str(i)]))\n",
    "        elif activation == 'relu':\n",
    "            dZ = np.multiply(dA, relu_derivative(cache['Z'+str(i)]))\n",
    "        else:\n",
    "            print('Incorrect activation value, process should abort.')\n",
    "        \n",
    "        dW = 1/m * np.matmul(dZ, cache['A'+str(i-1)].T)\n",
    "        db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        gradients['dW'+str(i)] = dW\n",
    "        gradients['db'+str(i)] = db        \n",
    "        \n",
    "        dA = np.matmul(parameters['W'+str(i)].T, dZ)\n",
    "    \n",
    "    return gradients\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Implementing tanh function\n",
    "def tanh_activation(Z):\n",
    "    A = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "# Derivative of tanh function\n",
    "def tanh_derivative(Z):\n",
    "    derivative = 1 / np.power(np.cosh(Z), 2)\n",
    "    return derivative\n",
    "\n",
    "\n",
    "# Implementing relu function\n",
    "def relu_activation(Z):\n",
    "    A = np.maximum(Z, np.zeros((Z.shape[0], Z.shape[1])))\n",
    "    return A\n",
    "\n",
    "\n",
    "# Derivative of relu function\n",
    "def relu_derivative(Z):\n",
    "    derivative = np.maximum(Z, np.zeros((Z.shape[0], Z.shape[1])))\n",
    "    derivative[derivative > 0] = 1\n",
    "    return derivative\n",
    "\n",
    "\n",
    "# Implementing sigmoid function\n",
    "def sigmoid_activation(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f62a6",
   "metadata": {},
   "source": [
    "# Test my implementation\n",
    "* Dataset used is a simple binary classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3887e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration is 0 and the current cost is 13.14706679078232\n",
      "Current iteration is 1 and the current cost is 13.14706463189791\n",
      "Current iteration is 2 and the current cost is 13.147062473015618\n",
      "Current iteration is 3 and the current cost is 13.147060314135445\n",
      "Current iteration is 4 and the current cost is 13.147058155257394\n",
      "Current iteration is 5 and the current cost is 13.14705599638147\n",
      "Current iteration is 6 and the current cost is 13.147053837507675\n",
      "Current iteration is 7 and the current cost is 13.147051678636009\n",
      "Current iteration is 8 and the current cost is 13.147049519766478\n",
      "Current iteration is 9 and the current cost is 13.147047360899082\n",
      "Current iteration is 10 and the current cost is 13.147045202033825\n",
      "Current iteration is 11 and the current cost is 13.147043043170711\n",
      "Current iteration is 12 and the current cost is 13.147040884309742\n",
      "Current iteration is 13 and the current cost is 13.14703872545092\n",
      "Current iteration is 14 and the current cost is 13.147036566594245\n",
      "Current iteration is 15 and the current cost is 13.147034407739723\n",
      "Current iteration is 16 and the current cost is 13.147032248887356\n",
      "Current iteration is 17 and the current cost is 13.147030090037147\n",
      "Current iteration is 18 and the current cost is 13.1470279311891\n",
      "Current iteration is 19 and the current cost is 13.147025772343214\n"
     ]
    }
   ],
   "source": [
    "## Read data\n",
    "data_file = 'data/binary_classification.csv'\n",
    "raw_data = pd.read_csv(data_file)\n",
    "\n",
    "# Extract X and scale\n",
    "X = raw_data.iloc[:, 1:raw_data.shape[1]]\n",
    "X_max = X.apply(np.max, axis=0)\n",
    "X_min = X.apply(np.min, axis=0)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "np_X = X.to_numpy().T\n",
    "np_X = np_X.astype('float64')\n",
    "\n",
    "# map Y to 0/1 binary values\n",
    "Y = raw_data.iloc[:, 0]\n",
    "Y = Y.map({'M': 0, 'B': 1})\n",
    "np_Y = Y.to_numpy()\n",
    "np_Y = np_Y.reshape(1, np_Y.shape[0])\n",
    "np_Y = np_Y.astype('float64')\n",
    "\n",
    "# Run deep neural network\n",
    "parameters = deep_NN(np_X, np_Y, [5, 1], max_iter=20, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae3df642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a20040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
