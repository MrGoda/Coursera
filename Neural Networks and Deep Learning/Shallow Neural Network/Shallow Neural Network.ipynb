{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f9b04e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "## Function declaration of shallow neural network\n",
    "# Hyperparameters include the learning rate and the dimension of the hidden layer\n",
    "def shallow(X, Y, layers_dim, learning_rate=0.001, max_iter=2000):\n",
    "    # Initialization\n",
    "    parameters = initialize(layers_dim)\n",
    "    \n",
    "    # Iterate a number of max_iter times\n",
    "    for i in range(max_iter):\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(X, parameters)\n",
    "        # Backward propagation\n",
    "        gradients = backward_prop(X, Y, parameters, cache)\n",
    "        # Update parameters\n",
    "        parameters['W1'] = parameters['W1'] - learning_rate * gradients['dW1']\n",
    "        parameters['b1'] = parameters['b1'] - learning_rate * gradients['db1']\n",
    "        parameters['W2'] = parameters['W2'] - learning_rate * gradients['dW2']\n",
    "        parameters['b2'] = parameters['b2'] - learning_rate * gradients['db2']\n",
    "\n",
    "    return parameters\n",
    "    \n",
    "\n",
    "# Function to initialize the four parameters\n",
    "def initialize(layers_dim):\n",
    "    #print(layers_dim)\n",
    "    W1 = np.random.randn(layers_dim[1], layers_dim[0]) * 0.01\n",
    "    b1 = np.zeros((layers_dim[1], 1))\n",
    "    W2 = np.random.randn(layers_dim[2], layers_dim[1]) * 0.01\n",
    "    b2 = np.zeros((layers_dim[2], 1))\n",
    "    parameters = {'W1': W1, \n",
    "                  'b1': b1, \n",
    "                  'W2': W2, \n",
    "                  'b2': b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Function to conduct forward propagation\n",
    "# Input: X and parameters\n",
    "# Output: The output of different layers\n",
    "def forward_prop(X, parameters):\n",
    "    # Extract parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    # Calculate output for layers\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "    cache = {'Z1': Z1, \n",
    "             'A1': A1,\n",
    "             'Z2': Z2, \n",
    "             'A2': A2}\n",
    "    return cache\n",
    "    \n",
    "    \n",
    "# Function to conduct backward propagation\n",
    "# Input: X, Y, and cache\n",
    "# Output: The gradients for the parameters\n",
    "def backward_prop(X, Y, parameters, cache):\n",
    "    # Extract info\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    # Calculate gradients\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2)))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    gradients = {'dW1': dW1, \n",
    "                 'db1': db1, \n",
    "                 'dW2': dW2, \n",
    "                 'db2': db2}\n",
    "    return gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "84ec8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "## Read data\n",
    "data_file = 'data/binary_classification.csv'\n",
    "raw_data = pd.read_csv(data_file)\n",
    "\n",
    "# Extract X and scale\n",
    "X = raw_data.iloc[:, 1:raw_data.shape[1]]\n",
    "X_max = X.apply(np.max, axis=0)\n",
    "X_min = X.apply(np.min, axis=0)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "np_X = X.to_numpy().T\n",
    "np_X = np_X.astype('float64')\n",
    "\n",
    "# map Y to 0/1 binary values\n",
    "Y = raw_data.iloc[:, 0]\n",
    "Y = Y.map({'M': 0, 'B': 1})\n",
    "np_Y = Y.to_numpy()\n",
    "np_Y = np_Y.reshape(1, np_Y.shape[0])\n",
    "np_Y = np_Y.astype('float64')\n",
    "\n",
    "# Run shallow neural network\n",
    "parameters = shallow(np_X, np_Y, [np_X.shape[0], 10, 1])\n",
    "\n",
    "# Test using the current set\n",
    "W1 = parameters['W1']\n",
    "b1 = parameters['b1']\n",
    "W2 = parameters['W2']\n",
    "b2 = parameters['b2']\n",
    "\n",
    "Z1 = np.dot(W1, np_X) + b1\n",
    "A1 = np.tanh(Z1)\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = 1 / (1 + np.exp(-Z2))\n",
    "y_pred = A2.copy()\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred\n",
    "\n",
    "count = 0\n",
    "for i in range(y_pred.shape[1]):\n",
    "    if y_pred[0][i] == np_Y[0][i]:\n",
    "        count += 1\n",
    "print(count/np_Y.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
